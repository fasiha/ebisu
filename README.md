# Ebisu: intelligent quiz scheduling

- [Ebisu: intelligent quiz scheduling](#ebisu-intelligent-quiz-scheduling)
  - [Introduction](#introduction)
  - [Install](#install)
  - [API Quickstart](#api-quickstart)
    - [Data model](#data-model)
    - [Predict recall probability](#predict-recall-probability)
    - [Update after a quiz](#update-after-a-quiz)
    - [How long till a model reaches some probability?](#how-long-till-a-model-reaches-some-probability)
  - [How it works](#how-it-works)
  - [Math](#math)
  - [Bibliography](#bibliography)
  - [Acknowledgments](#acknowledgments)


## Introduction
- [Literate document](https://fasiha.github.io/ebisu/)
- [GitHub repo](https://github.com/fasiha/ebisu)
- [PyPI package](https://pypi.python.org/pypi/ebisu/)
- [Changelog](https://github.com/fasiha/ebisu/blob/gh-pages/CHANGELOG.md)
- [Contact](https://fasiha.github.io/#contact)

Consider a student memorizing a set of facts.

- Which facts need reviewing?
- How does the student‚Äôs performance on a review change the fact‚Äôs future review schedule?

Ebisu is an open-source public-domain library that answers these two questions. It is intended to be used by software developers writing quiz apps, and provides a simple API to deal with these two aspects of scheduling quizzes, centered on two functions:
- `predictRecall` gives the current recall probability for a given fact.
- `updateRecall` adjusts the belief about future recall probability given a quiz result.

Behind this simple API, Ebisu is using a simple yet powerful model of forgetting, a model that is founded on Bayesian statistics and sum-of-exponentials (power law) forgetting.

With Ebisu, quiz applications can move away from ‚Äúdaily review piles‚Äù caused by less flexible scheduling algorithms. For instance, a student might have only five minutes to study today, so an app using Ebisu can ensure that only the facts most in danger of being forgotten are reviewed. And since every flashcard always has a recall probability at any given time, Ebisu also enables apps to provide an infinite stream of quizzes for students who are cramming. Thus, Ebisu intelligently handles over-reviewing as well as under-reviewing.

This document contains both a detailed mathematical description of the underlying algorithm as well as the software API it exports. Separate implementations in other languages are detailed below.

The next sections are installation and an [API Quickstart](#qpi-quickstart). See these if you know you want to use Ebisu in your app.

Then in the [How It Works](#how-it-works) section, I contrast Ebisu to other scheduling algorithms and describe, non-technically, why you should use it.

Then there‚Äôs a long [Math](#the-math) section that details Ebisu‚Äôs algorithm mathematically. If you like Gamma-distributed random variables, importance sampling, and maximum likelihood, this is for you.

> Nerdy details in a nutshell: Ebisu largely follows Mozer et al.‚Äôs multiscale context model (MCM) of `n` leaky integrators, published at NIPS 2009 (see [bibliography](#bibliography)), but with a Bayesian twist. The probability of recall for a given fact is assumed to be governed by an ensemble of decaying exponentials with *fixed* time constants (these increase from an hour to ten years) but *uncertain* mixture weights. The weights themselves decay according to an exponential to a single uncertain value governed by a Beta random variable. Therefore, the recall probability at any given time is a straightforward arithmetic expression of elapsed time, time constants, and weights. And after a quiz, the new best estimate of the weights is computed via a simple MAP (maximum a posteriori) estimator that uses a standard Scipy hill-climbing algorithm.
 
Finally, in the [Source Code](#source-code) section, we describe the software testing done to validate the math, including tests comparing Ebisu‚Äôs output to Monte Carlo sampling.

A quick note on history‚Äîmore information is in the [Changelog](https://github.com/fasiha/ebisu/blob/gh-pages/CHANGELOG.md). This document discusses Ebisu v3. Versions 2 and before used a very different model that was both more complex and that failed to handle the *strenghening* of memory that accompanied quizzes. If you are interested, see the [Changelog](https://github.com/fasiha/ebisu/blob/gh-pages/CHANGELOG.md) for details and a migration guide.

## Install
```sh
python -m pip install ebisu
```

## API Quickstart

### Data model
```py
def initModel(
    wmaxMean: Optional[float] = None,
    hmin: float = 1,
    hmax: float = 1e5,
    n: int = 10,
    initHlMean: Optional[float] = None,
    now: Optional[float] = None,
) -> Model
```
For each fact in your quiz app, create an Ebisu `Model` via `initModel`. The optional keyword arguments, `wmaxMean`, `hmin`, `hmax`, and `n`, govern the collection of leaky integrators (weighted exponentials) that are at the heart of the Ebisu framework. Let‚Äôs talk about how they work.

1. There are `n` leaky integrators (decaying exponentials), each with a halflife that‚Äôs strictly logarithmically increasing, starting at `hmin` hours (default 1) and ending at `hmax` hours (default `1e5` or roughly 11 years).
2. Each of the `n` leaky integrators also has a weight, indicating its maximum recall probability at time 0. The weights are strictly exponentially decreasing: the first leaky integrator gets a weight of 1 and the `n`th gets `wmaxMean`. A single leaky integrator predicts a recall probability \\(p_i(t) ‚àù w_i ‚ãÖ 2^{-t / h_i}\\) (here \\(t\\) indicates hours since last review, and \\(w_i\\) and \\(h_i\\) are this leaky integrator‚Äôs weight and halflife; the index \\(i\\) runs from 1 to `n`).

Putting these together, you get the Ebisu formula for probability of recall, just take the *max* of each leaky integrator: \\(p(t) = \max_i(w_i ‚ãÖ 2^{-t / h_i})\\).

For example, with `n=5` leaky integrators going from `hmin=1` hour to `hmax=1e4` hours (13.7-ish months), and with the last longest-duration exponential getting a weight of `wmaxMean=0.1`, we have this profile of weights:

![Weights per halflife for each leaky integrator](leaky-integrators-weights.png)

The next plot shows each of the five leaky integrators‚Äô contribution to the recall probability as well as the max among them at any given time, indicating the expected probability of recall‚Äî

![Recall probability for each leaky integrator, with max](leaky-integrators-precall.png)

At the left-most part of the plot, the first leaky integrator dominates, but very quickly fades away due to the crush of its fast exponential. As it decays however, the subsequent leaky integrator, with a strictly lower starting value, steps up to keep the recall probability from entirely collapsing. And so on till the last leaky integrator.

Switching the above plot‚Äôs x and y scales to log-log gives and zooming out to see more time gives us this:

![Recall probability, as above, on log-log plot](leaky-integrators-precall-loglog.png)

By taking the *max* of each the output of each leaky integrator, we get this *sequence* of bumps which roughly follow the ubiquitous memory *power law*, for times between 6 minutes and 1+ year. A true power law would, in a log-log plot such as this, be a straight line, and as `n` and `hmax` increase, the bumpy line representing the probability of recall above can be expected to converge to a power law (proof by visualization üôÉ). See Mozer et al. (cited above, and [bibliography](#bibliography)) for discussion and references suggesting the power-law decay of memory.

In this example, after more than a year (10‚Å¥ hours) since review, the probability of recall gets crushed by the exponential decay of the last leaky integrator. This can be avoided by using higher `hmax`, and this is why the default `hmax=1e5`, i.e., 11.4-ish years.

Having said *all* this, the most import input to `initModel` is `wmaxMean`, a number between 0 and 1 that represents the weight of the final `n`th leaky integrator and therefore governs all other weights. However, I think it‚Äôs safe to assume that you have *no* interest thinking about this strange random variable, and rather you have a *lot* of thoughts on this fact‚Äôs *halflife*. Therefore, `initModel` also lets you specify `initHlMean`, your best guess as to this fact‚Äôs initial memory halflife, in hours.

> Math note: we don‚Äôt ask for a full prior on ‚Äú`wmax`‚Äù, just its mean. This is because Ebisu deals naively pretends that \\(E[f(W)] ‚âà f(E[W])\\) (where \\(W\\) is the random variable representing the final leaky integrator‚Äôs weight) in order to efficiently and conveniently compute the recall probabilities (see `predictRecall` below!).
> 
> Math note 2: if you just provide `initHlMean`, we convert this to `wmaxMean` through a quick function minimization (see `hoursForModelDecay` below for how we convert an Ebisu `Model` to the halflife).

`now` is milliseconds since the Unix epoch (midnight UTC, Jan 1, 1970). Provide this to customize when the student learned this fact, otherwise Ebisu will use the current time.

You can serialize this `Model` with the `to_json` method provided by [Dataclasses-JSON](https://github.com/lidatong/dataclasses-json), which also provides its complement, `from_json`. Therefore, this will work:
```py
ebisu.Model.from_json(ebisu.initModel(0.1).to_json())
```

It's expected that apps using Ebisu will save the serialized JSON to a database. The model contains all historic quiz information and numbers describing the probabilistic configuration.

### Predict recall probability
```py
def predictRecall(
    model: Model,
    now: Optional[float] = None,
    logDomain=True,
) -> float
```
This functions answers one of the core questions any flashcard app asks: what fact is most in danger of being forgotten? You can run this function against all Ebisu models, with an optional `now` (milliseconds in the Unix epoch), to get a log-probability of recall. A higher number implies more likely to recall; the lower the number, the more risk of of forgetting.

If you pass in `logDomain=False`, this function will call `exp2` to convert log-probability (-‚àû to 0) to actual probability (0 to 1). (This is not done by default because `exp2`, floating-point power, is actually expensive compared to arithmetic. No, I don‚Äôt have an explicit reference. Yes, profiling is important.)

**Nota bene** if you‚Äôre storing Ebisu models as JSON in SQL, you might not need this function! The following snippet selects all columns and a new column, called `logPredictRecall`, assuming a SQLite table called `mytable` with Ebisu models in a column called `model_json`:
```sql
SELECT
  t.id,
  t.model_json,
  MAX(
    (
      JSON_EXTRACT(value, '$[0]') - (
        (?) - JSON_EXTRACT(model_json, '$.pred.lastEncounterMs')
      ) / JSON_EXTRACT(value, '$[1]')
    )
  ) AS logPredictRecall
FROM
  mytable t,
  JSON_EACH(JSON_EXTRACT(t.model_json, '$.pred.forSql'))
GROUP BY t.id
```
The placeholder `(?)` is for you to pass in the current timestamp (milliseconds since Unix epoch; in SQLite you can get this via `strftime('%s','now') * 1000`).

### Update after a quiz
```py
def updateRecall(
    model: Model,
    successes: Union[float, int],
    total: int = 1,
    q0: Optional[float] = None,
    wmaxPrior: Optional[tuple[float, float]] = None,
    now: Optional[float] = None,
) -> Model
```
The other really important question flashcard apps ask is: ‚ÄúI've done a quiz, now what?‚Äù This `updateRecall` function is for this crucial step.

Via `updateRecall`, Ebisu supports **two** distinct kinds of quizzes:
- with `total=1` you get *noisy-binary* (or *soft-binary*) quizzes where `0 <= successes <= 1` can be a float. This supports the most basic flashcard reviews (binary quizzes) but also some pretty complex workflows!
- With `total>1` you get *binomial* quizzes, meaning out of a `total` number of points the student could get, she got `successes` (must be integer).

Example 1. For the bog-standard flashcard review, where you show the student a flashcard and they get it right (or wrong), you can pass in `successes=1` (or `successes=0`), and use the default `total=1`. You get binary quizzes.

Example 2. For a Duolingo-style review, where you review the same fact multiple times in a single short quiz session, you provide the number of `successes` and `total>1` (the number of points received versus the maximum number of points, both integers). Ebisu treats this as a binomial quiz. (Math note: a binary quiz is just a special case of the binomial with `total=1`.)

Example 3. For more complex apps, where you have deep probabilistic insight into the student‚Äôs performance, you can specify noisy-binary quizzes by passing in `total=1` and `0 < successes < 1`, a float between 0 and 1. In this situation, we assume the existence of a ‚Äúreal‚Äù binary quiz result, which we *don‚Äôt* observe, and which was scrambled by going through a noisy channel that flipped the ‚Äúreal‚Äù quiz result with some probability.
- `max(successes, 1 - successes)` is `Probability(observed pass | real quiz pass)` and
- `q0` is `Probability(observed pass | real quiz failed)`, defaults to the complement of the above (`1 - max(successes, 1 - successes)`).

More concretely, imagine you have a foreign language reader app where users can read text and click on a word to look it up in the dictionary if they‚Äôve forgotten it. Now imagine you know the student read a word and they did *not* ask for its definition. You can‚Äôt say for sure that the student would have gotten it right if you‚Äôd *actually* prompted them for the definition of the word, so you don‚Äôt want to treat this as a normal ‚Äúsuccessful‚Äù quiz. Here you can say
- `Probability(did not ask for definition | they know the word) = successes = 1.0`, i.e., if they know the word, they would never ask for the definition (or maybe they would? Fine, make this 0.98), but
- `Probability(did not ask for definition | they forgot the word) = q0 = 0.1`: if they actually had forgotten the word, there‚Äôs a low but non-zero chance of observing the same behavior (didn‚Äôt ask for the definition).

With `successes`, `total`, and `q0`, Ebisu can handle a rich range of quiz results robustly and quantitatively.

This update function is what performs the full Bayesian analysis to estimate a new ‚Äú`wmax`‚Äù, the final leaky integrator‚Äôs weight. An important part of Bayesian analysis is your prior belief on what this value should be, before you‚Äôve looked at the data (the actual quiz results). You can provide `wmaxPrior`, a 2-tuple \\((Œ±, Œ≤)\\) representing the Beta distribution (we follow [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution)‚Äôs definition) representing your prior for this weight. This is optional‚Äîif you don‚Äôt provide `wmaxPrior`, we will find the highest-variance Beta distribution that implies a halflife equal to the student‚Äôs *maximum* inter-quiz interval. In practice, this works well, and follows Lindsey, et al. (see [bibliography](#bibliography)) in applying ‚Äúa bias that additional study in a given time window helps, but has logarithmically diminishing returns‚Äù (2014). (Lindsey, et al., is the same team those MCM (multiscale context model) NIPS 2009 paper I cite above as the core inspiration for Ebisu.)

As with other functions above, `updateRecall` also accepts `now`, milliseconds since the Unix epoch.

### How long till a model reaches some probability?
```
def hoursForRecallDecay(model: Model, percentile=0.5) -> float
```
This is sometimes useful for quizzes that seek to schedule a review in the future when a fact‚Äôs memory is expected to have decayed to some probability. This `hoursForRecallDecay`, in converting probability to time (hours), is sort of the inverse of `predictRecall` which converts time (hours) to probability. By default the probability is 0.5, so this function returns the halflife of a `Model`.

That‚Äôs it. Four functions in the API.

## How it works

There are many flashcard scheduling schemes, e.g.,

- [Anki](https://apps.ankiweb.net/), an open-source Python flashcard app (and a closed-source mobile app),
- the [SuperMemo](https://www.supermemo.com/help/smalg.htm) family of algorithms ([Anki‚Äôs](https://faqs.ankiweb.net/what-spaced-repetition-algorithm.html) is a derivative of SM-2),
- [Memrise.com](https://www.memrise.com), a closed-source webapp,
- [Duolingo](https://www.duolingo.com/) has published a [blog entry](http://making.duolingo.com/how-we-learn-how-you-learn) and a [conference paper/code repo](https://github.com/duolingo/halflife-regression) on their half-life regression technique,
- the Leitner and Pimsleur spacing schemes (also discussed in some length in Duolingo‚Äôs paper).
- Also worth noting is Michael Mozer‚Äôs team‚Äôs Bayesian multiscale models, specifically Mozer et al. (2009) and, by the same team, Lindsey et al. (2014) (see [bibliography](#bibliography)).

Memory research began with Hermann Ebbinghaus‚Äô discovery of the [forgetting curve](https://en.wikipedia.org/w/index.php?title=Forgetting_curve&oldid=766120598#History), published in 1885, when he was thirty-five. He [memorized random](https://en.wikipedia.org/w/index.php?title=Hermann_Ebbinghaus&oldid=773908952#Research_on_memory) consonant‚Äìvowel‚Äìconsonant trigrams (‚ÄòPED‚Äô, e.g.) and found, among other things, that his recall decayed logarithmically. More recent research has shown, apparently conclusively, that *forgetting* follows a power law decay.

Anki and SuperMemo are extremely popular. They use carefully-tuned mechanical rules to schedule a fact‚Äôs future review immediately after its current review. The rules can get complicated‚ÄîI wrote a little [field guide](https://gist.github.com/fasiha/31ce46c36371ff57fdbc1254af424174) to Anki‚Äôs, with links to the source code‚Äîsince they are optimized to minimize daily review time while maximizing retention. However, because each fact has simply a date of next review, these algorithms do not gracefully accommodate over- or under-reviewing. Even when used as prescribed, they can schedule many facts for review on one day but few on others. (I must note that all three of these issues‚Äîover-reviewing (cramming), under-reviewing, and lumpy reviews‚Äîhave well-supported solutions in Anki by tweaking the rules and third-party plugins.)

Duolingo‚Äôs half-life regression explicitly models the probability of you recalling a fact as an exponential, \\(2^{-Œî/h}\\) where Œî is the time since your last review and \\(h\\) is a *half-life*. In this model, your chances of passing a quiz after \\(h\\) days is 50%, which drops to 25% after \\(2 h\\) days, and so on. They estimate this half-life by combining your past performance and fact metadata in a large-scale machine learning technique called half-life regression (a variant of logistic regression or beta regression, more tuned to this forgetting curve). With each fact associated with a half-life, they can predict the likelihood of forgetting a fact if a quiz was given right now. The results of that quiz (for whichever fact was chosen to review) are used to update that fact‚Äôs half-life by re-running the machine learning process with the results from the latest quizzes.

The Mozer group‚Äôs algorithms (MCM (their 2009 paper) and DASH (their 2014 paper; see [bibliography](#bibliography))) also curve-fit a large quantity of quiz data to high-dimensional models, including, in DASH‚Äôs case, a hierarchical Bayesian model that takes into account inter-fact and inter-student variability.

Ebisu is like Duolingo and Mozer‚Äôs algorithms, in that it explicitly tracks the recall probability as it decays. Ebisu further adapts the mathematical form of the memory decay after Mozer et al.‚Äôs MCM (multiscale context model): a cascade of weighted exponentials.

However, Ebisu adds a few Bayesian twists to these approaches:
1. Ebisu posits an explicit probability distribution on the parameters of the memory decay. This allows you to treat your subjective belief about each flashcard‚Äôs difficulty as a Bayesian prior that governs its memory decay over time as well as its memory strengthening via reviews.
2. We also support a rich variety of quizzes fully analytically: 
    - binary quizzes‚Äîpass/fail,
    - binomial quizzes‚Äîe.g., three points out of four,
    - even exotic noisy-binary quizzes that let you fully specify the odds of the student ‚Äúpassing‚Äù the quiz when they actually don‚Äôt know the answer (handy for deweighting multiple-choice vs. active recall, as well as for reader apps described above).
3. Both predicting memory *and* incorporating quizzes are done computationally-efficiently on a quiz-by-quiz basis. Once ported to JavaScript or Kotlin or Swift, the algorithm can readily run on your phone, no need for large sets of historic quiz data and expensive training. The most you need in terms of computational mathematics is a one-dimensional function minimization (e.g., golden section search) and the Gamma function.

Note that Ebisu treats each flashcard‚Äôs memory as independent of the others. It can‚Äôt handle flashcard correlation or interference, alas, so you have to handle this in your application.

The hope is that Ebisu can be used by flashcard apps that continue to unleash the true potential of personalized learning and spaced reptition practice. 

With the [API docs](#api-quickstart) above and this explanation, let‚Äôs jump into a more formal description of the mathematics.

## Math
(Forthcoming.)

## Bibliography

While most citations are given inline above, this section contains academic papers, to whose PDFs I want to provide multiple links.

Lindsey, R. V., Shroyer, J. D., Pashler, H., & Mozer, M. C. (2014). Improving Students‚Äô Long-Term Knowledge Retention Through Personalized Review. <cite>Psychological Science</cite>, 25(3), 639‚Äì647. [DOI](https://doi.org/10.1177/0956797613504302), [academic copy](https://home.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/LindseyShroyerPashlerMozer2014Published.pdf), [local copy](./LindseyShroyerPashlerMozer2014Published.pdf). The authors also share some very interesting mathematical details as ‚ÄúAdditional Methods‚Äù under [Supplemental Material](https://journals.sagepub.com/doi/10.1177/0956797613504302#supplementary-materials) on SagePub.

Michael C. Mozer, Harold Pashler, Nicholas Cepeda, Robert Lindsey, and Ed Vul. 2009. Predicting the optimal spacing of study: a multiscale context model of memory. In <cite>Proceedings of the 22nd International Conference on Neural Information Processing Systems (NIPS'09)</cite>. Curran Associates Inc., Red Hook, NY, USA, 1321‚Äì1329. [DOI](https://dl.acm.org/doi/10.5555/2984093.2984242), [academic copy](https://home.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/MozerPashlerCepedaLindseyVul2009.pdf), [local copy](./MozerPashlerCepedaLindseyVul2009.pdf).


## Acknowledgments

A huge thank you to [bug reporters and math experts](https://github.com/fasiha/ebisu/issues?utf8=%E2%9C%93&q=is%3Aissue) and [contributors](https://github.com/fasiha/ebisu/graphs/contributors)!

Many thanks to [mxwsn and commenters](https://stats.stackexchange.com/q/273221/31187) as well as [jth](https://stats.stackexchange.com/q/272834/31187) for their advice and patience with my statistical incompetence.

Many thanks also to Drew Benedetti for reviewing this manuscript.

John Otander‚Äôs [Modest CSS](http://markdowncss.github.io/modest/) is used to style the Markdown output.
