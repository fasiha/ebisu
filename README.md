# Ebisu: intelligent quiz scheduling

- [Ebisu: intelligent quiz scheduling](#ebisu-intelligent-quiz-scheduling)
  - [Introduction](#introduction)
  - [Install](#install)
  - [API quickstart](#api-quickstart)
  - [How it works](#how-it-works)
  - [The Math](#the-math)
    - [Exponential decay](#exponential-decay)
    - [Noisy-binary quizzes](#noisy-binary-quizzes)
    - [Power laws](#power-laws)
    - [Leaky integrators: recall probability](#leaky-integrators-recall-probability)
    - [Recall probability in SQL](#recall-probability-in-sql)
    - [Leaky integrators: update recall](#leaky-integrators-update-recall)
  - [Source code](#source-code)
  - [Bibliography](#bibliography)
  - [Acknowledgments](#acknowledgments)


## Introduction
- [Literate document](https://fasiha.github.io/ebisu/)
- [GitHub repo](https://github.com/fasiha/ebisu)
- [PyPI package](https://pypi.python.org/pypi/ebisu/)
- [Changelog](https://github.com/fasiha/ebisu/blob/gh-pages/CHANGELOG.md)
- [Contact](https://fasiha.github.io/#contact)

Consider a student memorizing a set of facts.

- Which facts need reviewing?
- How does the student‚Äôs performance on a review change the fact‚Äôs future review schedule?

Ebisu is an open-source public-domain library that answers these two questions. It is intended to be used by software developers writing quiz apps, and provides a simple API to deal with these two aspects of scheduling quizzes, centered on two functions:
- `predictRecall` gives the recall probability for a given fact at any given timestamp.
- `updateRecall` adjusts the belief about future recall probability given a quiz result.

Behind this simple API, Ebisu is using a simple yet powerful model of forgetting, a model that is founded on Bayesian statistics and sum-of-exponentials (power law) forgetting. Thanks to these probabilistic foundations, Ebisu is able to handle quite a rich variety of quiz types:
- of course you have your binary quizzes, i.e., pass/fail;
- you also have Duolingo-style quizzes where the student got X points out of a maximum of Y points (binomial quizzes);
- you can even customize the probability that the student ‚Äúpassed‚Äù the quiz even if they forgot the fact‚Äîthis is handy for deweighting multiple-choice quizzes, and also for reader apps where the readers can click on words they don‚Äôt know (‚Äúthe dog that didn‚Äôt bark‚Äù).

So with Ebisu, quiz applications can move away from ‚Äúdaily review piles‚Äù caused by less flexible scheduling algorithms. For instance, a student might have only five minutes to study today, so an app using Ebisu can ensure that only the facts most in danger of being forgotten are reviewed. And since every flashcard always has a recall probability at any given time, Ebisu also enables apps to provide an infinite stream of quizzes for students who are cramming. Thus, Ebisu intelligently handles over-reviewing as well as under-reviewing.

Ebisu also has been able to support creative quiz apps that move beyond simple pass/fail flashcards.

So. This document contains both a detailed mathematical description of the underlying algorithm as well as the software API it exports. Separate implementations in other languages are detailed below.

The next sections are installation and an [API Quickstart](#qpi-quickstart). See these if you know you want to use Ebisu in your app.

Then in the [How It Works](#how-it-works) section, I contrast Ebisu to other scheduling algorithms and describe, non-technically, why you should use it.

Then there‚Äôs a long [Math](#the-math) section that details Ebisu‚Äôs algorithm mathematically. If you like nonlinear-transformed Gamma-distributed random variables, modified Bessel functions of the second kind, and incomplete Gamma functions, this is for you.

> Nerdy details in a nutshell: Ebisu posits that your memory for a flashcard decays according to a power law, which it models with a *sequence* of weighted exponentials with halflives following Gamma random variables. Your probability of recall at any given time is the *maximum* of this array weighted exponentials (what Mozer et al. call ‚Äúleaky integrators‚Äù (see [bibliography](#bibliography))), and which we approximate with a simple arithmetic expression that can be run even in [SQL](#recall-probability-in-sql). Next, a *quiz* is treated as Bernoulli, binomial, or a neat ‚Äúnoisy-binary‚Äù trial; after the Bayesian update, each halflife‚Äôs posterior is moment-matched to the closest Gamma random variable. Based on the strength of the posterior update, the weights for each halflife are updated.

Finally, in the [Source Code](#source-code) section, we describe the software testing done to validate the math, including tests comparing Ebisu‚Äôs output to Monte Carlo sampling.

A quick note on history‚Äîmore information is in the [Changelog](https://github.com/fasiha/ebisu/blob/gh-pages/CHANGELOG.md). This document discusses Ebisu v3, with its Gammas-on-halflives model. Versions 2 and before used a different model (a Beta-on-recall-probability) which didn‚Äôt capture the fact that memory is *strengthened* by review‚Äîit just viewed future recall probability as an *unknown but static* quantity, leading to pessimistic predictions of recall probability. If you are interested, see the [Changelog](https://github.com/fasiha/ebisu/blob/gh-pages/CHANGELOG.md) for details and a migration guide.

## Install
```sh
python -m pip install ebisu
```

## API quickstart
> This is intended to be a quick refresher for those already familiar with the Ebisu API. If it doesn't make sense, jump to the full [API](#api-and-math) section!

**Step 0.** `import ebisu`

**Step 1.** Create an Ebisu `Model` for each flashcard when a student learns it:
```py
def initModel(
    halflife: Optional[float] = None,  # hours
    finalHalflife=1e5,  # hours
    n: int = 10,
    weightsHalflifeGammas: Optional[list[tuple[float, HalflifeGamma]]] = None,
    now: Optional[float] = None,
) -> Model
```
If you want to do the minimal amount of work to create a model, just provide `halflife` in hours. This is your best guess of how long it will take for this flashcard‚Äôs memory to decay to 50% (the ‚Äúhalf‚Äù in ‚Äúhalflife‚Äù).

This will create a sequence of `n=10` decaying exponentials (‚Äúleaky integrators‚Äù in Mozer et al.‚Äôs terminology) whose halflives are Gamma random variables. The means of these Gammas are logarithmically-spaced from some fraction of `halflife` to `finalHalflife` of 1e5 hours (more than 11 years), and their standard deviations are some proportion of the mean. This will also assign each of these Gamma random variables a weight that logarithmically-decreases from 1.0 such that the overall probability of recall has your provided `halflife`.

If you dislike any of the above defaults, you can tune them using the keyword arguments, or, for total control, pass in `weightsHalflifeGammas`, a list of tuples containing each leaky integrator‚Äôs weight and `HalflifeGamma` (a tuple of `alpha, beta` parameters of each Gamma random variable).

`now` is when this fact was learned (milliseconds in the Unix epoch, midnight UTC on 1 January, 1970). If you don‚Äôt provide it, the current timestamp is used.

**Step 2.** Find the `Model` with the lowest recall probability. You can do this in SQL (see [below](#recall-probability-in-sql)!) or use:
```py
def predictRecall(
    model: Model,
    now: Optional[float] = None,
    logDomain=True,
) -> float
```
`now` is again milliseconds since the Unix epoch started. If omitted, the current timestamp is used. By default this returns the *log* of the recall probability (from -‚àû to 0, higher is more likely to recall). If you pass in `logDomain=False`, we will call `exp2` at the end to give you linear probability from 0 to 1. We keep the calculations in the log-domain to avoid numerical issues and by default give you the log-probability only because `exp2` (and in general powers of any base) are slow on most CPUs compared to arithmetic.

**Step 3.** After you show the student a flashcard and grade their answer, update the `Model`:
```py
def updateRecall(
    model: Model,
    successes: Union[float, int],
    total: int = 1,
    q0: Optional[float] = None,
    now: Optional[float] = None,
) -> Model
```
This is a pure function: the input `Model` is left untouched, so you can replace it with the returned `Model`. It supports two quiz modes:
- a binary/binomial quiz is denoted by integer `successes` (points received) out of `total` points possible. 
- A noisy-binary quiz is implied by `total=1` and uses `0 < successes < 1`, a float and optionally `q0` to specify its parameters.

`q0 = P(successful quiz | actually forgot)`, that is, the probability that the quiz was successful given the student actually has actually forgotten the fact. Notionally there exists `q1 = P(successful quiz | actually remembers)`, the probability of successful quiz given the student truly remembers the fact, and is independent of `q0`. But `q1` is taken to be `max(successes, 1 - successes)`. So `successes=0.1` and `successes=0.9` both encode the `q1 = 0.9` case but overall quiz fail and pass, respectively. (See the [math section](#noisy-binary-quizzes) for more details, I know this can be confusing!)

`now` is as before milliseconds in the Unix epoch.

**Bonus** It can be useful to know when a `Model`‚Äôs memory will decay to some probability:
```py
def hoursForRecallDecay(model: Model, percentile=0.5) -> float
```
With the default `percentile=0.5`, the returned value corresponds to the halflife (in hours).

## How it works

There are many flashcard scheduling schemes, e.g.,

- [Anki](https://apps.ankiweb.net/), an open-source Python flashcard app (and a closed-source mobile app),
- the [SuperMemo](https://www.supermemo.com/help/smalg.htm) family of algorithms ([Anki‚Äôs](https://faqs.ankiweb.net/what-spaced-repetition-algorithm.html) is a derivative of SM-2),
- [Memrise.com](https://www.memrise.com), a closed-source webapp,
- [Duolingo](https://www.duolingo.com/) has published a [blog entry](http://making.duolingo.com/how-we-learn-how-you-learn) and a [conference paper/code repo](https://github.com/duolingo/halflife-regression) on their half-life regression technique,
- the Leitner and Pimsleur spacing schemes (also discussed in some length in Duolingo‚Äôs paper).
- Also worth noting is Michael Mozer‚Äôs team‚Äôs Bayesian multiscale models, specifically Mozer et al. (2009) and, by the same team, Lindsey et al. (2014) (see [bibliography](#bibliography)).

Memory research began with Hermann Ebbinghaus‚Äô discovery of the [forgetting curve](https://en.wikipedia.org/w/index.php?title=Forgetting_curve&oldid=766120598#History), published in 1885, when he was thirty-five. He [memorized random](https://en.wikipedia.org/w/index.php?title=Hermann_Ebbinghaus&oldid=773908952#Research_on_memory) consonant‚Äìvowel‚Äìconsonant trigrams (‚ÄòPED‚Äô, e.g.) and found, among other things, that his recall decayed logarithmically. More recent research has shown, apparently conclusively, that *forgetting* follows a power law decay.

Anki and SuperMemo are extremely popular flashcard apps. They use carefully-tuned mechanical rules to schedule a fact‚Äôs future review immediately after its current review. The rules can get complicated‚ÄîI wrote a little [field guide](https://gist.github.com/fasiha/31ce46c36371ff57fdbc1254af424174) to Anki‚Äôs, with links to the source code‚Äîsince they are optimized to minimize daily review time while maximizing retention. However, because each fact has simply a date of next review, these algorithms do not gracefully accommodate over- or under-reviewing. Even when used as prescribed, they can schedule many facts for review on one day but few on others. (I must note that all three of these issues‚Äîover-reviewing (cramming), under-reviewing, and lumpy reviews‚Äîhave well-supported solutions in Anki by tweaking the rules and third-party plugins.)

Duolingo‚Äôs half-life regression explicitly models the probability of you recalling a fact as an exponential, $2^{-Œî/h}$ where Œî is the time since your last review and $h$ is a *half-life*. In this model, your chances of passing a quiz after $h$ days is 50%, which drops to 25% after $2 h$ days, and so on. They estimate this half-life by combining your past performance and fact metadata in a large-scale machine learning technique called half-life regression (a variant of logistic regression or beta regression, more tuned to this forgetting curve). With each fact associated with a half-life, they can predict the likelihood of forgetting a fact if a quiz was given right now. The results of that quiz (for whichever fact was chosen to review) are used to update that fact‚Äôs half-life by re-running the machine learning process with the results from the latest quizzes.

The Mozer group‚Äôs algorithms (MCM (their 2009 paper) and DASH (their 2014 paper; see [bibliography](#bibliography))) also curve-fit a large quantity of quiz data to high-dimensional models, including, in DASH‚Äôs case, a hierarchical Bayesian model that takes into account inter-fact and inter-student variability.

Like Duolingo and Mozer et al., Ebisu explicitly tracks a forgetting curve for each flashcard, enabling it to sort a list of flashcards from most to least likely to be forgotten. However, Ebisu formulates the problem very differently‚Äîwhile memory is understood to decay according to a specific formula, Ebisu posits *probability distributions* on the halflives governing this decay and uses quiz results to update its beliefs about those halflives in a fully Bayesian way. These updates, while a bit more computationally-burdensome than Anki‚Äôs scheduler, are much lighter-weight than Duolingo‚Äôs industrial-strength approach.

Being Bayesian grants Ebisu some really useful features:
1. Quiz app authors can encode quite specific beliefs about their students‚Äô initial memory of a flashcard. If you have a reason to believe that vocabulary item A is much more easy to learn than item B, you can specify that exactly and quantitatively.
2. You can *always* ask, ‚Äúwhat is the expected recall probability of this fact‚Äù. Therefore you can *always* sort a set of flashcards from most likely to be forgotten to least likely.
3. Being Bayesian lets us support a rich variety of quizzes fully analytically: 
    - binary quizzes‚Äîpass/fail,
    - binomial quizzes‚Äîe.g., three points out of four,
    - even exotic noisy-binary quizzes that let you fully specify the odds of the student ‚Äúpassing‚Äù the quiz when they actually don‚Äôt know the answer (handy for deweighting multiple-choice vs. active recall, as well as for reader apps described above).

Note that Ebisu treats each flashcard‚Äôs memory as independent of the others. It can‚Äôt handle flashcard correlation or interference, alas, so you have to handle this in your application.

The hope is that Ebisu can be used by flashcard apps that continue to unleash the true potential of personalized learning and spaced reptition practice. 

Now let‚Äôs jump into a more formal description of the mathematics and the resultant Ebisu Python API.

## The Math
### Exponential decay
While much psychological literature has identified that forgetting follows power-law decay (e.g., probability of recall $t$ time units after last review $p(t) = (t+1)^{-Œ±}$ for some positive shape parameter $Œ±$), we start by discussing a simpler case, exponential decay, because we will use a string of exponentially-decaying functions to approximate a power law.

So let‚Äôs imagine a flashcard with halflife in hours $h ‚àº \mathrm{Gamma}(Œ±, Œ≤)$, that is, a Gamma random variable with known parameters $Œ±$ and $Œ≤$.

This flashcard‚Äôs probability of recall after $t$ hours since last encountering it is
$$p(t) = 2^{-t/h}.$$
At the halflife, the recall probability $p(h) = 0.5$ has decayed to half-strength. At twice the halflife, $p(2 h) = 0.25$, and so on.

For any given time elapsed $t$, we can compute the expected value of this recall probability via the law of the unconscious statistician ([LOTUS](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician)), whereby for random variable $X$, $E[g(X)] = ‚à´_{-‚àû}^‚àû g(x) f_X(x) \,\mathrm{d}x$, that is, the expectation of a function ($g$) of a random variable involves integrating that function and the random variable‚Äôs density ($f_X$). In our case we use the Gamma‚Äôs density:
$$
E\left[p(t) = 2^{-t/h}\right] = \frac{Œ≤^Œ±}{Œì(Œ±)}  ‚à´_0^‚àû 2^{-t/h} h^{Œ± - 1} e^{Œ≤ h} \,\mathrm{d}h.
$$
This is not as ferocious as it first looks. Since $2^x = e^{x \log 2}$ (where $\log$ is the natural log, with base $e$), we can absorb terms, and then let <a name="sympy-integral"></a>Sympy do all the real work: it turns out that for positive constants $a$, $b$, and $c$,
$$
  ‚à´_0^‚àû h^{a - 1} e^{-b h - c / h} \,\mathrm{d}h = 2 \left(\frac{c}{b}\right)^{a/2} K_{a}(2\sqrt{b c}).
$$
$K_ŒΩ(z)$ here is the modified Bessel function of the second kind with order $ŒΩ$ (lol which is Greek letter ‚Äúnu‚Äù) and argument $z$. Frankly, I don‚Äôt know much about this function but [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.kv.html) provides it‚Äîlucky!

> For completeness, note that when $c=0$ (which can happen in our application when the time elapsed since last quiz is $t=0$), there‚Äôs a simpler solution. The integrand is just the Gamma distribution‚Äôs density, so $‚à´_0^‚àû h^{a-1} e^{-b h} \,\mathrm{d}h = b^{-a}Œì(a)$, i.e., the reciprocal of the normalizing constant in the Gamma density.

<a name="expectation-p-recall"></a>Therefore, we have
$$
  E\left[p(t) \right] = \frac{2 Œ≤^Œ±}{Œì(Œ±)} \left(\frac{t \log 2}{Œ≤}\right)^{Œ±/2} K_{Œ±}(2\sqrt{Œ≤ t \log 2}),
$$
which could be simplified a bit more but I‚Äôll leave it like this because it uses the result of the Sympy integral above, which we‚Äôll have occasion to invoke later.

Nota bene, Ebisu doesn‚Äôt actually use this expectation anywhere since it never has a single Gamma-distributed halflife (only a series of them). I‚Äôve belabored this derivation mainly because it introduces some results we‚Äôll use next‚Äîquizzes.

So. Suppose at time $t$ hours since last review, we obtain a  binomial quiz:
$$k|h ‚àº \mathrm{Binomial}(n, p(t) = 2^{-t/h})$$
In words: the student got $k$ points out of $n$ total where each point was independent and had probability $p(t)$. (For $n=1$ of course the binomial trial simplifies to a Bernoulli trial, i.e., a binary quiz.)

What is the posterior, $P(h | k)$? By Bayes, we have
$$P(h|k) = \frac{P(k|h) P(h)}{\int_0^‚àû P(k|h) P(h) \,\mathrm{d}h}$$
In words: the posterior is proportional to the likelihood $P(k|h)$ (the binomial distribution‚Äôs probability mass function, PMF) scaled by the prior $P(h)$ (the Gamma distribution‚Äôs probability density function, PDF), normalized by a constant to ensure everything sums to 1.

Let‚Äôs rewrite things in terms of base $e$ to consolidate. The numerator
$$
\begin{split}
P(k|h)P(h) &‚àù \left(e^{-(\log 2)  t/h}\right)^k \left(1-e^{-(\log 2) t/h}\right)^{n-k} h^{Œ± - 1} e^{-Œ≤ h}
\\
  &‚àù \left(1-e^{-(\log 2) t/h}\right)^{n-k} h^{Œ± - 1} e^{-Œ≤ h - k(\log 2) t/h}.
\end{split}
$$
Recall the ‚Äú$‚àù$‚Äù symbol is ‚Äúproportional to‚Äù, and it lets us drop constants that will cancel out anyway when we normalize by the denominator above.

This is the form of the posterior $P(h|k)$, and we will eventually want to compute the moments of this (its mean and variance) in order to moment-match to a new Gamma random variable and treat that as the *new* prior for the *next* quiz. To get there, let‚Äôs define a series of useful constants: for integer $N‚â•0$,
$$
m_N = ‚à´_0^‚àû h^N ‚ãÖ  \left(1-e^{-(\log 2) t/h}\right)^{n-k} h^{Œ± - 1} e^{-Œ≤ h - k(\log 2) t/h} \,\mathrm{d}h.
$$
(Note how $m_0$ is simply equal to the normalizing denominator of the overall posterior, hint hint.) Despite appearing fearsome, this is actually close to being manageable. The major annoyance is that $(1-e^{-(\log 2)t/h})^{n-k}$ term. We can simplify that via the [binomial theorem](https://en.wikipedia.org/w/index.php?title=Binomial_theorem&oldid=944317290#Theorem_statement): in general,
$$
  ‚à´_0^‚àû f(x) (1-g(x))^n \, \mathrm{d}x = \sum_{i=0}^{n} \left[ (-1)^i \binom{n}{i} ‚à´_0^‚àû g(x)^i ‚ãÖ f(x) \, \mathrm{d}x \right],
$$
that is, we can replace the polynomial in $m_N$‚Äôs integrand with a sum:
$$
m_N = ‚àë_{i=0}^{n-k} \left[
  (-1)^i \binom{n-k}{i}
  ‚à´_0^‚àû h^{Œ± + N - 1} e^{-Œ≤ h - (k-i)(\log 2) t / h} \,\mathrm{d}h
\right].
$$
Note that the inner integral is in the same form as the one we solved with Sympy [above](#sympy-integral), for the expected probability of recall. So the constant is
$$m_N = ‚àë_{i=0}^{n-k}
  (-1)^i \binom{n-k}{i}
  2 \left(\frac{(k-i)(\log 2) t}{Œ≤}\right)^{(Œ±+N)/2} K_{Œ±+N}(2\sqrt{Œ≤  (k-i)(\log 2) t})
.$$
This is‚Ä¶ good! I promise! We‚Äôll see later how this is implemented quite carefully in Python using `gammaln` and `kve` and `logsumexp` to maintain numerical accuracy but for now note how this is really useful because the moments of the posterior $P(h|k)$ follow very straightforwardly from the way we‚Äôve constructed $m_N$. The mean
$$E[h|k] = Œº = \frac{1}{m_0} ‚à´_0^‚àû h \left(1-e^{-(\log 2) t/h}\right)^{n-k} h^{Œ± - 1} e^{-Œ≤ h - k(\log 2) t/h}$$
is simply $Œº = \frac{m_1}{m_0}$. The second non-central moment $E[(h|k)^2] = \frac{m2}{m_0}$, so the variance is $œÉ^2 = m_2/m_0 - Œº^2$.

With the mean and variance of the posterior $h|k$ in place, we can moment-match to a Gamma random variable with the same mean and variance: that is, with
- $Œ±' = Œº^2/ œÉ^2$ for mean $Œº$ and variance $œÉ^2$ as above, and
- $Œ≤' = Œº / œÉ^2$,

$\mathrm{Gamma}(Œ±', Œ≤')$ is a reasonable approximation for the true posterior halflife.

Let‚Äôs stop and take stock of where we are and how we got here.
1. We began with a prior on the halflife of a fact: in hours, $h ‚àº \mathrm{Gamma}(Œ±, Œ≤)$ for known parameters $Œ±$ and $Œ≤$.
2. After $t$ hours since last seeing this fact, the student underwent a quiz and got $k$ points out of a possible $n$. We treated $k ‚àº \mathrm{Binomial}(k; n, 2^{-t/h})$, as a binomial trial with underlying probability $2^{-t/h}$.
3. We went through the Bayesian machinery to obtain the moments of the posterior $h | k$, that is, our new belief about the distribution of the fact‚Äôs halflife in light of the quiz.
4. With those moments, we created a new distribution $\mathrm{Gamma}(Œ±', Œ≤')$ which matches the true posterior in mean and variance. We can now return to step 1!

To get some feel for this behavior, consider two models‚Äîtwo Gamma random variables‚Äîwith the same mean, seven hours. One has standard deviation of 4.5 and the other 10.5 hours‚Äîcall these the ‚Äúlow œÉ‚Äù and ‚Äúhigh œÉ‚Äù models, respectively. Below we see the mean posterior after a binary $n=1$ quiz for a range of times $t$ (hours after last review).<a name="binom-updates-plot"></a>

![Pass vs fail, high vs low œÉ](./binom-updates.png)

This shows some nice properties of Ebisu:
- failures strictly reduce the halflife while successes strictly increase it.
- However, a failure far beyond the halflife barely budges the halflife: we expected that quiz to fail, so it isn‚Äôt that informative about our belief of the true halflife.
- Similarly, very early successes also only slightly increase the halflife, for the same reason.
- Conversely, early *failures* and late *successes* result in large posterior updates because they are so unexpected.
  - Do note that this isn‚Äôt coded up anywhere. This is simply the result of the Bayesian framework doing it‚Äôs thing: updating prior beliefs in light of new data.
- The high œÉ model encodes more uncertainty about the halflife so it gets a far more aggressive update in either direction compared to the low œÉ model, whose prior is more insistent on the true halflife being seven hours.
  - This too is a crucial aspect of Bayesian methods: your prior is your anchor. If you have strong reasons to believe that a flashcard has a certain halflife, you can encode this in the Œ± and Œ≤ of the Gamma distribution you pick. If your belief about the true halflife is only vague, this is valuable information too‚Äîin this situation, the Bayesian formulation naturally will elevate data over prior.

Compare this binary case to the binomial $n=2$ case, for the low œÉ model:

![Comparing the binary quiz case to the n=2 binomial cases](./binom-n2-updates.png)

The left plot shows the same two lines as the low œÉ model above. On the right, we show three curves, for $k=2$ to $k=0$.

Only the $k=0$ case, i.e., zero points out of max two, has the property that its posterior mean halflife remains strictly below the prior‚Äôs mean (seven hours). And similarly the $k=2$ case‚Äôs posterior mean is strictly *above* the prior mean.

For $k=1$, however, the posterior mean is less than or greater than the prior mean, depending on how much time elapsed the quiz and the last encounter with this fact. It‚Äôs also bounded between the pass and fail curves of the binary case.

Furthermore, the $k=0$ and $k=2$ cases lead to much more pronounced updates than the binary $n=1$ model‚Äôs results on the left. Therefore, the binomial $n>1$ updates might be used by quiz apps to potentially encode ‚Äúdifficulty‚Äù. While ad hoc, a ‚Äúvery easy‚Äù or ‚Äúvery hard‚Äù binary quiz could be encoded as a $k=2$ or $k=0$ binomial trial for $n=2$, respectively.

Now. Let‚Äôs take a quick detour to look at how to handle the other quiz type Ebisu supports‚Äîthe noisy-binary quiz‚Äîbefore seeing how to use this exponential decay to construct power-law decay.

### Noisy-binary quizzes
Can we imagine a quiz type where the student could score 0 or 1 point (out of max 1) but also somewhere in between? As luck would have it, Stack Exchange user [@mef](https://stats.stackexchange.com/a/419320) has invented a lovely way to model this and it is quite useful in capturing some advanced quiz types.

Let $x ‚àº \mathrm{Bernoulli}(p(t))$ be a ‚Äútrue‚Äù Bernoulli draw representing the answer to the question, ‚Äúdoes the student *really* know this fact?‚Äù ($p(t)=2^{-t/h}$ here is the same exponential recall probability as before.)

But this random variable is hidden from us: we don‚Äôt observe $x$. Rather, we observe a quiz, a ‚Äúnoisy report‚Äù, $z|x ‚àº \mathrm{Bernoulli}(q_x)$ where
- $q_1 = P(z = 1 | x = 1)$, that is, the probability of us observing a successful quiz when the student *really* knows the fact, while
- $q_0 = P(z = 1 | x = 0)$, i.e., the probability of us observing a successful quiz when the student has in fact forgotten the fact.

In signal processing terms, the true but hidden result $x$ goes through a noisy channel, which might flip the bit, and what we observe is the output of that channel, $z$.

In the plain binary case without fuzziness, $q_1 = 1$ and $q_0 = 0$, but in the soft-binary case, these two parameters are independent and free for you to specify as any numbers between 0 and 1 inclusive.

Given $h‚àº\mathrm{Gamma}(Œ±, Œ≤)$ and known $Œ±$, $Œ≤$, $q_1$, and $q_0$, we can ask what the posterior $h | z$ is. We can use the fact that the likelihood is
$$
\begin{split}
P(z|h) &= P(z|x) ‚ãÖ P(x|h)
  \\
  &= \mathrm{Bernoulli}(z; q_x) ‚ãÖ \mathrm{Bernoulli}\left(x; p(t)=2^{-t/h}\right).
\end{split}
$$
Then, as before,
$$
P(h|z) = \frac{P(z|h)P(h)}{‚à´_0^‚àû P(z|h)P(h) \,\mathrm{d}h},
$$
and this time we can break things up into the two cases, where $z=1$ vs $z=0$:
$$
P(z|h)P(h) ‚àù  \begin{cases}
   h^{Œ± - 1} e^{-Œ≤ h} \left( (q_1 - q_0)e^{-(\log2) t / h} + q_0\right) &\text{if } z=1 \\
   h^{Œ± - 1} e^{-Œ≤ h} \left( (q_0 - q_1)e^{-(\log2) t / h} + (1-q_0)\right) &\text{if } z=0
\end{cases}
$$
These expressions are the Gamma density and the expansion of the likelihood $P(z|h)$, a product of two Bernoulli probability mass functions.

As before, we can define a sequence of pseudo-moments $m_N$ for integer $N‚â•0$ for the noisy-binary case‚Äîthe calculus we derived for the binomial quiz case above helps us immensely and we just give the result here:
$$
m_N = 2 \left(\frac{(\log 2) t}{Œ≤}\right)^{(Œ±+N)/2} K_{Œ±+N}\left(2\sqrt{Œ≤(\log 2) t}\right) ‚ãÖ r_z + s_z \frac{Œì(Œ±+N)}{Œ≤^{Œ± + N}}
$$
for constants
$$
r_z = \begin{cases}
q_1 - q_0 &\text{if } z=1 \\
q_0 - q_1 &\text{if } z=0
\end{cases}
$$
and
$$
s_z = \begin{cases}
  q_0 &\text{if } z=1 \\
1-q_0 &\text{if } z=0.
\end{cases}
$$
(You can verify that these simplify to the binary quiz case, i.e., the binomial $n=1$ case, for $q_0=0$ and $q_1=1$!)

The rest is the same as before, from the binomial quiz case. The mean (the first moment) of the posterior $E[h|z]=Œº=\frac{m_1}{m_0}$ while the second non-central moment is $E[(h|z)^2]=\frac{m_2}{m_0}$, yielding a variance that is $œÉ^2 = \frac{m_2}{m_0} - Œº^2$. This mean and variance can again be moment-matched to a new $\mathrm{Gamma}(Œ±' =  Œº^2/ œÉ^2, Œ≤' = Œº / œÉ^2)$.

We should note here that both $q_1 = P(z = 1 | x = 1)$ and $q_0 = P(z = 1 | x = 0)$ are *free* parameters, and apps have total flexibility in specifying these. In Ebisu‚Äôs API presented [above](#api-quickstart) (step 3), both $z$ and $q_1$ are encoded without loss of generality in `0 <= successes <= 1`:
- $z=1$ if `successes > 0`, otherwise $z=0$.
- $q_1$ is `max(successes, 1 - successes)`.

Therefore if `successes = 0.1`, then we know $z=0$ and $q_1 = 0.9$.

$q_0$ is provided in a keyword argument and for the sake of developer experience, $q_0=1-q_1$ is picked as a default when none is provided.

> While this choice for the default $q_0$ is ad hoc, it does have the nice property that `successes` between 0 and 1 will smoothly and symmetrically (around 0.5) scale the posterior halflife between the binary fail/pass cases. Also, as a satisfying bonus, a *totally* uninformative quiz with  `successes = 0.5` results in *no change* to the prior, i.e., $Œ±' = Œ±$ and $Œ≤' = Œ≤$!

### Power laws
So we‚Äôve derived the mathematical specifics of two quiz styles for the *exponential* forgetting model, where our beliefs about the halflife of a fact $h$ are converted to recall probabilities via $p(t) = 2^{-t/h}$, even though considerable research has shown forgetting is a power-law phenomenon. In this section, we will show how a staggered sequence of exponentials leads us to the power law we‚Äôd like to govern Ebisu recall.

To set the stage first‚Äîrecall that exponential decay, where the time factor $t$ is in the exponent, is truly *fundamentally* different than power-law decay where $t$ is in the base. $2^{-t}$ decays *incredibly* quickly‚Äîyou will recall what Einstein apparently said about exponentials being the most powerful force in the universe. After seven halflives, the probability of recall has dropped less than 1%: $2^{-7} = 1/128$. Meanwhile, power laws decay much more slowly: $(t+1)^{-1}$ has the same halflife as $2^{-t}$ (both have decayed to 0.5 at $t=1$) but after seven halflives, the probability of recall for the power law is still $1/8$, i.e., 12.5%, an order of magnitude higher than 0.8%!

Mozer et al. in both their 2009 NIPS conference paper and their 2014 <cite>Psychological Science</cite> journal article (see [bibliography](#bibliography)) propose a model recall that uses a *series* of exponentials, which they call a ‚Äúcascade of leaky integrators‚Äù. Here‚Äôs what that could look like, for a notional example: we have a sequence of five decaying exponentials whose halflives are logarithmically-spaced from one hour to `1e4` hours (1.1 years), and each exponential is scaled by a exponentially-decreasing weight, from 1.0 down. Here‚Äôs a plot of these five weighted exponentials, superimposed on which is a line (thick, dotted, gray) representing the *max* of any of these at any point in time:

![Recall probability for each leaky integrator, with max](leaky-integrators-precall.png)

At the left-most part of the plot, the first leaky integrator with the shortest time constant (blue solid line) dominates but also very quickly fades away due to the crush of its fast exponential. But as it decays, the second leaky integrator (orange dotted line), with a strictly lower starting value/weight, steps up to keep the recall probability from collapsing. And so on till the last leaky integrator.

Switching the above plot‚Äôs x and y scales to log-log gives and zooming out to see more time gives us this:

![Recall probability, as above, on log-log plot](leaky-integrators-precall-loglog.png)

By taking the *max* of the output of each leaky integrator, we get this *sequence* of bumps, which describe a bumpy power law for times ranging between minutes to 1+ year. A *true* power law would, in a log-log plot such as this, be a straight line for all time‚Äînot only is ours bumpy, it‚Äôs also flat at either end (horizontal on the left, vertical on the right). At very short intervals, this makes sense‚Äîwe can‚Äôt have probability of recall exceed 1‚Äîbut for times beyond a year, after we‚Äôve run out of leaky integrators, the probability of recall collapses quickly to zero under the crush of the exponential (Einstein‚Äôs most powerful force in the universe and all that).

Nevertheless, this is very encouraging. Instead of probability of recall $p(t) = 2^{-t/h}$ for a *single* unknown halflife which we discussed previously, we can consider 
$$p(t) = \max_{i=1}^{n} \left\lbrace w_i ‚ãÖ 2^{-t/h_i} \right\rbrace$$
for $h_i ‚àº \mathrm{Gamma}(Œ±_i, Œ≤_i)$. That is, we have $n$ Gamma-distributed random variables with known parameters $(Œ±_i, Œ≤_i)$ whose means $Œº_i = \frac{Œ±_i}{Œ≤_i}$ are logarithmically-spaced between a low and high of our choice (e.g., one hour to `1e4` hours in the plot above).

We also assume that $n$ weights, $w_i$, are known and deterministic‚Äîthis is a modeling choice to keep Ebisu‚Äôs posterior inference lightweight in a manner that seems psychologically plausible. So while we need $w_i ‚àà [0, 1]$, we are otherwise quite free in choosing these weights. Mozer et al. (2009, see [bibliography](#bibliography)) propose exponentially-spaced weights, which we also use, giving us
$$w_i = (w_n)^{\frac{i-1}{n-1}}$$
for some fixed final weight $w_n$, thereby setting the first weight on our shortest/fastest leaky integrator $w_1 = 1$.

> Note that I have assiduously avoided calling our model a mixture model, because I don‚Äôt want the weights to sum to one and we don‚Äôt want the overall recall probability to be a weighted mean (something like $``‚àë_{i=1}^n \tilde w_i 2^{-t/h_i}"$ where $\tilde w_i = w_i / ‚àë_{i=1}^n w_i$). While such a mixture model would also yield a power law, and has a more convenient expression for the expected value of recall probability, we have a gentle preference for the `max` operator (and weights starting with $w_1=1$ and decreasing) because it lets the weight-update step (below) be simpler and more robust. (It's plausible that in the future we will switch to a mixture model.)

### Leaky integrators: recall probability

So let us take stock.
- We have $n$ different Gamma-distributed halflives, $h_i ‚àº \mathrm{Gamma}(Œ±_i, Œ≤_i)$, and
- $n$ corresponding weights $w_i = (w_n)^\frac{i-1}{n-1}$, for $i$ running from 1 to $n$.
- We assume $n$, all $Œ±_i$ and $Œ≤_i$ and $w_i$ are known deterministic quantities.
- And we have the probability of recall $p(t) = \max_i  \lbrace w_i ‚ãÖ 2^{-t/h_i} \rbrace$.

Let us answer the two questions at the heart of Ebisu‚Äîwhat‚Äôs the recall probability at time $t$ and what does a quiz do?

So. What is $E[p(t) = \max_i  \lbrace w_i ‚ãÖ 2^{-t/h_i} \rbrace]$, the expected value of recall probability $t$ hours after last seeing this flashcard? The $\max$ makes this impossible to compute this analytically (it would be easier with a mixture model admittedly). üò≠. 

<details>
<summary>(Why this expectation is hard)</summary>

> To see why this is hard, consider that in general, if you have $n$ random variables $X_i$ for $i$ running from 1 to $n$, and take $X = \max_{i=1}^n X_i$, the probability density function (PDF) of the max $X$ can be found by first noticing that its cumulative distribution function (CDF) is
> $$F_X(x) = P[X \leq x] = P[X_1\leq x,...,X_n\leq x] = ‚àè_{i=1}^n P[X_i\leq x] = ‚àè_{i=1}^n F_{X_i}(x).$$
> Differentiating the distribution (CDF) via [product rule](https://en.wikipedia.org/w/index.php?title=Product_rule&oldid=1119757448#Product_of_more_than_two_factors) and some massaging gives us the density (PDF),
> $$f_X(x) = ‚àë_{i=1}^n \left( f_{X_i}(x) \prod_{j\neq i} F_{X_j}(x) \right).$$
> Unhappily, because our $X_i = w_i 2^{-t/h_i}$, for Gamma-distributed $h_i$, the distribution of $w_i 2^{-t/h_i}$ is ü§∑, so we can‚Äôt get the expectation this way.
> 
> There‚Äôs another expression for this expectation, via [user76284](https://math.stackexchange.com/q/3679707) on the Mathematics Stack Exchange, but it‚Äôs just as hard:
> $$E[X] = 1 - ‚à´_{-‚àû}^‚àû ‚àè_{i=1}^n F_{X_i}(x) \,\mathrm{d}x.$$

</details>

But we have good approximations for this expectation $E[p(t)]$!

We know via [Jensen‚Äôs inequality](https://mathworld.wolfram.com/JensensInequality.html) that for random variable $X$ and some nonlinear function $f$, in general $E[f(X)] ‚â† f(E[X])$. You can‚Äôt just move the expectation inside a nonlinear expression‚Äîmy mnemonic for this is, for $X ‚àº \mathrm{Normal}(0, 1)$ the unit normal/Gaussian, $(E[X])^2 = 0$, but that‚Äôs very different from whatever $E[X^2]$ is!

But just consider how much computationally straightforward these successive approximations are!
- Exact: $E[\max_i \lbrace w_i ‚ãÖ 2^{-t/h_i} \rbrace]$, unknown üòµ.
- Semi-Bayesian: $\max_i \lbrace w_i ‚ãÖ E[2^{-t/h_i}] \rbrace$, doable! We calculated this inner expectation [above](#expectation-p-recall), and it needs Bessel and Gamma functions but doable!
- Full approximation: $\max_i \lbrace w_i ‚ãÖ 2^{-t/ E[h_i]} \rbrace$, trivial! We could do this in SQL (see [below](#predict-recall-sql))!

How bad are these approximations? Not too bad! The three choices are shown below for a notional model. The exact expectation computed via Monte Carlo (thick dotted blue) and describes a nice power law‚Äîthe random overlap between adjacent Gamma random variables smooths out the bumps we saw before. The two successive approximations wobble around the exact expectation, describing the bumps. Notice the semi-Bayesian approximation (thin solid orange line) matches the Monte Carlo curve at the very left and very right of the curve, when a single Gamma-distributed halflife dominates. 

![Monte Carlo vs semi-Bayesian vs full approximation of recall probability expectation](./predictRecall-approx.png)

Because of the immense computational attractiveness of the fully-approximated expectation (solid thick green line), where the expectation operator is moved all the way inside, i.e., $E[\max_i \lbrace w_i ‚ãÖ 2^{-t/h_i} \rbrace] ‚Üí \max_i \lbrace w_i ‚ãÖ 2^{-t/ E[h_i]} \rbrace$, and the fact that the approximation follows the overall shape of the exact expectation (apparently achievable only via Monte Carlo), Ebisu‚Äôs `predictRecall` function uses it.

### Recall probability in SQL
As promised, this can be done in SQL! Assuming you have a SQLite table `mytable` with a column `json_column` contaiing your JSON-encoded Ebisu models, the following returns row IDs, the JSON, and a third value called `logPredictRecall` below computing `predictRecall`.
```sql
SELECT
  t.id,
  t.json_column,
  MAX(
    (
      json_extract(value, '$[0]') - (
        (?) - json_extract(json_column, '$.pred.lastEncounterMs')
      ) / json_extract(value, '$[1]')
    )
  ) AS logPredictRecall
FROM
  mytable t,
  json_each(json_extract(t.json_column, '$.pred.forSql'))
GROUP BY t.id
```
Check the bundled script [`sql-example.py`](./sql-example.py) for a fully-worked example.

### Leaky integrators: update recall
In this section, we describe a simple way to extend the single-Gamma-distributed-halflife quizzes (the [binomial](#exponential-decay) and [noisy-binary](#noisy-binary-quizzes) cases above) to a series of $n$ leaky integrators, i.e., $n$ weighted Gamma random variables governing halflife. The approach is not probabilistically motivated and therefore is ad hoc, but it has some attractive properties to commend it.

So.
1. We start with $n$ leaky integrators, i.e., $h_i ‚àº \mathrm{Gamma}(Œ±_i, Œ≤_i)$ for $i$ from 1 to $n$ with all parameters known, as well as corresponding weights $w_i$, also known.
2. At time $t$ we get a binomial or noisy-binary quiz result with underlying probability $p(t) = \max_i \lbrace w_i ‚ãÖ 2^{-t/h_i} \rbrace$.
3. We then apply the appropriate quiz update to each leaky integrator, i.e., find $\mathrm{Gamma}(Œ±_i', Œ≤_i')$ that best approximate each posterior $h_i | \mathrm{quiz}$.
    - This gives us a scalar $Œî_i = Œº_i' / Œº_i$ where $Œº_i = Œ±_i / Œ≤_i$ and its primed version $Œº_i' = Œ±_i' / Œ≤_i'$ are the mean of the $i$th prior and posterior, respectively.
4. This is the magic (i.e., ad hoc) part: update the weights, $w_i'=\max(Œî_i w_i, 1)$.
    - (¬°Small print applies, see below!)


The scale factor $Œî_i > 0$ represents the strengthening or weakening ($Œî_i > 1$ or $Œî_i < 1$, respectively) of the posterior over the prior due to the quiz. More specifically, it is the change in our belief about the $i$th halflife as a *purely static quantity*: in the basic Bayesian analysis above ([¬ß binomial](#exponential-decay) and [¬ß noisy-binary](#noisy-binary-quizzes)), the random variable $h_i$ is unknown but *fixed*. However, we know that memory is actually strengthened through the act of recall, which the posterior update represented by $Œî_i$ fails to capture. 

By scaling the $i$th leaky integrator‚Äôs weight $w_i$ with $Œî_i$, we attempt to recapture this psychological reality. There is nothing Bayesian about this, and we use it because it‚Äôs an inexpensive way to quantitatively boost the leaky integrator to capture the dynamicity of the problem, and one that seems to work well for a variety of quiz histories and simulations.

However, we need some small print because the above simple weight update doesn‚Äôt work well as is (given that it‚Äôs totally ad hoc, that isn‚Äôt surprising). To see why, note that we don‚Äôt want to *always* scale the weights‚Äîconsider when a student first learns a flaschard, their memory is weak and they are likely to fail some initial quizzes. However, a long-halflife leaky integrator would find such failures very surprising and drastically reduce it‚Äôs posterior halflife (refer again to the [binary updates plot](#binom-updates-plot)). We don‚Äôt want this. We don‚Äôt want long-duration leaky integrators to be responding to very early quizzes. We want them to be there to prop up the probability of recall at long timescales (power law!), which is defeated by them becoming low-weight, short-timescale leaky integrators.

Similarly recall that a successful quiz with a very short amount of time elapsed since last review can result in a posterior mean that‚Äôs slightly higher than the prior. We would also prefer early *successes* to avoid ‚Äúpolluting‚Äù long-duration leaky integrators.

In short, we want a leaky integrator to smoothly kick in and begin updating itself with quizzes when quiz delays have reached and exceeded its halflife.

In practice, Ebisu keeps a boolean representing ‚Äúweight reached‚Äù for each leaky integrator. If the boolean is `True`, this leaky integrator has been reached and its weight should be updating per $w_i'=\max(Œî_i w_i, 1)$. Otherwise its $Œî_i$ is ignored and its weight *and* model $(Œ±_i, Œ≤_i)$ are left untouched. Initially only the first shortest-duration leaky integrator has its ‚Äúwegiths reached‚Äù flag set to `True`.

A leaky integrator‚Äôs ‚Äúweight reached‚Äù flag is set from `False` to `True` when
- $w_l ‚â§ Œî_i ‚â§ 1$ or
- $w_r ‚â§ Œî_i$.

Currently $w_l = 0.95$ (the ‚Äúleft‚Äù bound) and $w_r = 1.01$ (the ‚Äúright‚Äù bound).

Once the ‚Äúweights reached‚Äù flag is set `True`, it is never unset.

## Source code

## Bibliography

While most citations are given inline above, this section contains academic papers, to whose PDFs I want to provide multiple links.

Lindsey, R. V., Shroyer, J. D., Pashler, H., & Mozer, M. C. (2014). Improving Students‚Äô Long-Term Knowledge Retention Through Personalized Review. <cite>Psychological Science</cite>, 25(3), 639‚Äì647. [DOI](https://doi.org/10.1177/0956797613504302), [academic copy](https://home.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/LindseyShroyerPashlerMozer2014Published.pdf), [local copy](./LindseyShroyerPashlerMozer2014Published.pdf). The authors also share some very interesting mathematical details as ‚ÄúAdditional Methods‚Äù under [Supplemental Material](https://journals.sagepub.com/doi/10.1177/0956797613504302#supplementary-materials) on SagePub.

Michael C. Mozer, Harold Pashler, Nicholas Cepeda, Robert Lindsey, and Ed Vul. 2009. Predicting the optimal spacing of study: a multiscale context model of memory. In <cite>Proceedings of the 22nd International Conference on Neural Information Processing Systems (NIPS'09)</cite>. Curran Associates Inc., Red Hook, NY, USA, 1321‚Äì1329. [DOI](https://dl.acm.org/doi/10.5555/2984093.2984242), [academic copy](https://home.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/MozerPashlerCepedaLindseyVul2009.pdf), [local copy](./MozerPashlerCepedaLindseyVul2009.pdf).


## Acknowledgments

A huge thank you to [bug reporters and math experts](https://github.com/fasiha/ebisu/issues?utf8=%E2%9C%93&q=is%3Aissue) and [contributors](https://github.com/fasiha/ebisu/graphs/contributors)!

Many thanks to [mxwsn and commenters](https://stats.stackexchange.com/q/273221/31187) as well as [jth](https://stats.stackexchange.com/q/272834/31187) for their advice and patience with my statistical incompetence.

Many thanks also to Drew Benedetti for reviewing this manuscript.

John Otander‚Äôs [Modest CSS](http://markdowncss.github.io/modest/) is used to style the Markdown output.
